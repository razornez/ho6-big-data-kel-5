{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmRD4bJuWh0z"
   },
   "source": [
    "# HandsOn Week 6\n",
    "Welcome to HandsOn Week 6. In this HandsOn, you will try to play with spark streaming where the data is from a Kafka producer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYzgRfXhWh06"
   },
   "source": [
    "## Setting up\n",
    "Since there SparkStreaming from Kafka is not supported in Spark version 3.1.1, There are some things that you need to setup or install:\n",
    "1. You need to download apache spark version 2.7.4 with hadoop 2.7 [https://spark.apache.org/downloads.html]\n",
    "2. Unzip the tgz file\n",
    "3. Open bashrc file `nano ~/.bashrc`. Then, find those variables and set the values to\n",
    "    * SPARK_HOME=~/Downloads/spark-2.4.7-bin-hadoop2.7\n",
    "    * PYSPARK_PYTHON=python3.7\n",
    "4. Activate `source ~/.bashrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSnqBfJHWh08"
   },
   "source": [
    "## Milestone 1\n",
    "You need to install kafka-python by ```pip install kafka-python```.<br><br>\n",
    "In this milestone, you only need to run ```producer_variance.py``` and ```consumer_variance.py``` (these two code files are already provided inside the zip file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAszsXDDWh09"
   },
   "source": [
    "Screenshot your ```consumer_variance.py``` output, and put in this cell below. \n",
    "\n",
    "<img src=\"milestone1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IutlTG-hWh0-"
   },
   "source": [
    "## Milestone 2\n",
    "After making sure that the message is published by ```producer_variance.py``` and successfully consumed by ```consumer_variance.py``` in the topic of ```variance``` in the Milestone 1 above, then, you are ready for Milestone 2.<br>\n",
    "\n",
    "In Milestone 2, you need to implement ```calculate_variance``` function with the formula --> $variance = \\frac{\\sum_{i=1}^{N}x_i^2}{N}-(\\frac{\\sum_{i=1}^{N}x_i}{N})^2$. This function will be used to calculate variance for each window operation to the streaming data, and the variance is \"**accumulative/global**\" value up to current stream data. For example, in the first window, we have data ```1,2,3```, then the variance is the variance of ```1,2,3```. Let's say we have streaming data of ```4,5,6``` in the second window, thus the variance in this second window is the variance of ```1,2,3,4,5,6```, and so on for the following windows.<br>\n",
    "\n",
    "The ```calculate_variance``` function will return a DStream (RDD) with a format of ```('sum_x_square:', sum_x_square_value, 'sum_x:', sum_x_value, 'n:', n_value, 'var:', variance_value)``` where ```sum_x_square_value```$=\\sum_{i=1}^{N}x_i^2$, ```sum_x_value```$=\\sum_{i=1}^{N}x_i$ and ```n```$=N$. Note that $x_i=$ i-th of individual stream data, and $N=$ the number of individual stream data -count- up to i-th data.\n",
    "\n",
    "**Important:** In order to stream from Kafka producer to Spark Streaming, you need to download [spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar](https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.7) from maven repository (adjust with your Spark version), and put this jar file to ```your_spark_folder/jars```. For VM provided by the class, ```spark_folder``` is in ```/home/bigdata/spark-2.4.5-bin-hadoop2.7```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted code for easier running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oPzyakuvWh1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:17\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273.0, 'sum_x:', 63.0, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:19\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 364.0, 'sum_x:', 84.0, 'n:', 24, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:21\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 455.0, 'sum_x:', 105.0, 'n:', 30, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:23\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 546.0, 'sum_x:', 126.0, 'n:', 36, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:25\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 637.0, 'sum_x:', 147.0, 'n:', 42, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:27\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 728.0, 'sum_x:', 168.0, 'n:', 48, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:29\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 819.0, 'sum_x:', 189.0, 'n:', 54, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:31\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 910.0, 'sum_x:', 210.0, 'n:', 60, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:33\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1001.0, 'sum_x:', 231.0, 'n:', 66, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:35\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1092.0, 'sum_x:', 252.0, 'n:', 72, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:37\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1183.0, 'sum_x:', 273.0, 'n:', 78, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:39\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1274.0, 'sum_x:', 294.0, 'n:', 84, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:41\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1365.0, 'sum_x:', 315.0, 'n:', 90, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:43\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1456.0, 'sum_x:', 336.0, 'n:', 96, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:45\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1547.0, 'sum_x:', 357.0, 'n:', 102, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-02-27 08:17:47\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 1638.0, 'sum_x:', 378.0, 'n:', 108, 'var:', 2.916666666666666)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "KAFKA_TOPIC = \"variance\"\n",
    "BOOTSTRAP_SERVER = \"localhost:9092\"\n",
    "\n",
    "ssc = StreamingContext(sc,1) #stream each one second\n",
    "ssc.checkpoint(\"./checkpoint\")\n",
    "lines = KafkaUtils.createDirectStream(ssc, [KAFKA_TOPIC],\n",
    "                                      {\"metadata.broker.list\": BOOTSTRAP_SERVER})\n",
    "def calculate_variance(lines, window_length = 2, sliding_interval = 2):\n",
    "    \"\"\"\n",
    "    Function to calculate \"accumulated/global variance\" in each window operation\n",
    "    Params:\n",
    "        lines: Spark DStream defined above (in this jupyter cell)\n",
    "        window_length: length of window in windowing operation\n",
    "        sliding_interval: sliding interval for the window operation\n",
    "    Return:\n",
    "        result: DStream (RDD) of variance result with \n",
    "                format --> ('sum_x_square:', sum_x_square_value, 'sum_x:', sum_x_value, 'n:', n_value, 'var:', variance_value)\n",
    "                Example:   ('sum_x_square:', 182.0, 'sum_x:', 42.0, 'n:', 12.0, 'var:', 2.916666666666666)\n",
    "    \"\"\"\n",
    "    # Realize this function here. Note that you are not allowed to modify any code other than this function.\n",
    "    def update_function(new_values, global_values):\n",
    "        if global_values is None:\n",
    "            global_values = (0,0,0)\n",
    "        new_sum_x_square = sum([x[0] for x in new_values])\n",
    "        new_sum_x = sum([x[1] for x in new_values])\n",
    "        new_n = sum([x[2] for x in new_values])\n",
    "        global_sum_x_square, global_sum_x, global_n = global_values\n",
    "        return (new_sum_x_square + global_sum_x_square, new_sum_x + global_sum_x, new_n + global_n)\n",
    "    \n",
    "    parsed = lines.window(window_length, sliding_interval).flatMap(lambda x: x[1].split(\" \"))\n",
    "    reduced = parsed.map(lambda x: (1,(float(x)**2, float(x), 1)))\n",
    "    update = reduced.updateStateByKey(update_function)\n",
    "    transformed = update.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]))\n",
    "    result = transformed.map(lambda x: ('sum_x_square:', x[1][0], 'sum_x:', x[1][1], 'n:', x[1][2], 'var:', x[1][0]/x[1][2] - (x[1][1]/x[1][2])**2))\n",
    "    return result\n",
    "\n",
    "\n",
    "# run the function\n",
    "result = calculate_variance(lines, window_length=2, sliding_interval=2)\n",
    "# Print\n",
    "result.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iahW_8w2Wh1G"
   },
   "source": [
    "## Submission\n",
    "Archive this ipynb file and the screenshot image needed in the Milestone 1 into zip file with a format of: ```HandsOnWeek6_NIM_FullName.zip```, and submit to the submission form.\n",
    "\n",
    "**Note**: make sure in the Milestone 2, the cell has its output, but not too many streams (you can save this ipynb file approximatelly in the range of 4-20 window operations)\n",
    "\n",
    "Enjoy..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e9b8e52f303c93a13f8d6d945738d91211aad0184906d5189f93e8d39255a5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
